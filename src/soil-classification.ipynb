{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":102672,"databundleVersionId":12375409,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# -------------------------\n# Soil Image Classification Challenge\n# The Soil Image Classification Challenge is a machine learning competition organised by Annam.ai at IIT Ropar, serving as an initial task for shortlisted hackathon participants. Competitors will build models to classify each soil image into one of four categories: Alluvial soil, Black soil, Clay soil, or Red soil. Final submissions are due by May 25, 2025, 11:59 PM IST. Be sure to submit well before the deadline to avoid server overload or last-minute issues.\n# Task: Classify each provided soil image into one of the four soil types (Alluvial, Black, Clay, Red).\n# Deadline: May 25, 2025, 11:59 PM IST\n# Team Name: RootCoders (Amit Lakhera, Vikramjeet, Jyoti Ghungru, Pradipta Das, Sukanya Saha)\n# Last Modified: May 25, 2025\n# -------------------------\n\n# Import Required Libraries\nimport os\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n# PyTorch Libraries\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.models import resnet18, ResNet18_Weights\n\n# Sklearn for splitting and label encoding\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load and Prepare Data\ntrain_df = pd.read_csv('/kaggle/input/soil-classification/soil_classification-2025/train_labels.csv')\ntrain_df['image'] = train_df['image_id']\ntrain_df['label'] = train_df['soil_type']\n\nle = LabelEncoder()\ntrain_df['label_encoded'] = le.fit_transform(train_df['label'])\n\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['label_encoded'], random_state=42)\n\n# Image Transformations\nimg_size = 224\nmean = [0.485, 0.456, 0.406]\nstd  = [0.229, 0.224, 0.225]\n\ntrain_transforms = transforms.Compose([\n    transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((img_size, img_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)\n])\n\n# Custom Dataset Classes\nclass SoilDataset(Dataset):\n    def __init__(self, df, img_dir, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_name = self.df.loc[idx, 'image']\n        label = self.df.loc[idx, 'label_encoded']\n        img_path = os.path.join(self.img_dir, img_name)\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\ntrain_img_dir = '/kaggle/input/soil-classification/soil_classification-2025/train/'\ntrain_dataset = SoilDataset(train_df, train_img_dir, transform=train_transforms)\nval_dataset = SoilDataset(val_df, train_img_dir, transform=val_transforms)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n\n# Model Setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nweights = ResNet18_Weights.DEFAULT\nmodel = resnet18(weights=weights)\nmodel.fc = nn.Linear(model.fc.in_features, 4)\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.0005)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n\n# Evaluation Function\ndef evaluate_model(model, data_loader, criterion=None):\n    model.eval()\n    correct, total, val_loss = 0, 0, 0.0\n    with torch.no_grad():\n        for images, labels in data_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            if criterion:\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n    acc = correct / total\n    avg_loss = val_loss / len(data_loader) if criterion else None\n    return acc, avg_loss\n\n\n# Training Loop\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10):\n    best_acc = 0.0\n    best_model_state = None\n\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss, correct, total = 0.0, 0, 0\n\n        for images, labels in tqdm(train_loader):\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n        scheduler.step()\n        train_acc = correct / total\n        val_acc, val_loss = evaluate_model(model, val_loader, criterion)\n\n        if val_acc > best_acc:\n            best_acc = val_acc\n            best_model_state = model.state_dict()\n\n        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n                  f\"Loss: {running_loss:.4f}, \"\n                  f\"Train Acc: {train_acc:.4f}, \"\n                  f\"Val Loss: {val_loss:.4f}, \"\n                  f\"Val Acc: {val_acc:.4f}\")\n\n\n    torch.save(best_model_state, 'best_model.pth')\n    print(\"Best model saved with validation accuracy:\", best_acc)\n\n# Train the Model\ntrain_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=15)\n\n# Load Test Data and Create Submission\ntest_df = pd.read_csv('/kaggle/input/soil-classification/soil_classification-2025/test_ids.csv')\ntest_df['image'] = test_df['image_id']\ntest_img_dir = '/kaggle/input/soil-classification/soil_classification-2025/test/'\n\n# Define class TestSoilDataset\nclass TestSoilDataset(Dataset):\n    def __init__(self, df, img_dir, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_name = self.df.loc[idx, 'image']\n        img_path = os.path.join(self.img_dir, img_name)\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image, img_name\n\ntest_dataset = TestSoilDataset(test_df, test_img_dir, transform=val_transforms)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Generate Predictions\nmodel.load_state_dict(torch.load('best_model.pth'))\nmodel.eval()\npredictions = []\n\nwith torch.no_grad():\n    for images, image_names in tqdm(test_loader, desc=\"Validation\"):\n        images = images.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        predictions.extend(zip(image_names, predicted.cpu().numpy()))\n\nsubmission_df = pd.DataFrame(predictions, columns=['image_id', 'label_encoded'])\nsubmission_df['soil_type'] = le.inverse_transform(submission_df['label_encoded'])\nsubmission_df = submission_df[['image_id', 'soil_type']]\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(submission_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T17:08:11.493128Z","iopub.execute_input":"2025-05-24T17:08:11.493368Z","iopub.status.idle":"2025-05-24T17:10:19.126761Z","shell.execute_reply.started":"2025-05-24T17:08:11.493347Z","shell.execute_reply":"2025-05-24T17:10:19.125934Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 187MB/s]\n100%|██████████| 31/31 [00:07<00:00,  3.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/15], Loss: 14.0931, Train Acc: 0.8414, Val Loss: 0.7218, Val Acc: 0.7918\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 31/31 [00:05<00:00,  5.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/15], Loss: 6.8361, Train Acc: 0.9232, Val Loss: 0.1377, Val Acc: 0.9633\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 31/31 [00:05<00:00,  5.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/15], Loss: 6.5114, Train Acc: 0.9202, Val Loss: 0.3270, Val Acc: 0.9265\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 31/31 [00:05<00:00,  5.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/15], Loss: 6.5918, Train Acc: 0.9273, Val Loss: 0.1993, Val Acc: 0.9347\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 31/31 [00:05<00:00,  5.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/15], Loss: 5.3668, Train Acc: 0.9365, Val Loss: 0.2107, Val Acc: 0.9429\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 31/31 [00:05<00:00,  5.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/15], Loss: 3.2987, Train Acc: 0.9662, Val Loss: 0.1298, Val Acc: 0.9633\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 31/31 [00:06<00:00,  5.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/15], Loss: 2.4141, Train Acc: 0.9713, Val Loss: 0.1064, Val Acc: 0.9673\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 31/31 [00:05<00:00,  5.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/15], Loss: 1.9563, Train Acc: 0.9795, Val Loss: 0.1127, Val Acc: 0.9633\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 31/31 [00:06<00:00,  4.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/15], Loss: 1.5137, Train Acc: 0.9846, Val Loss: 0.1118, Val Acc: 0.9673\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 31/31 [00:05<00:00,  5.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/15], Loss: 1.7423, Train Acc: 0.9816, Val Loss: 0.1138, Val Acc: 0.9633\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 31/31 [00:06<00:00,  5.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [11/15], Loss: 1.2643, Train Acc: 0.9846, Val Loss: 0.1058, Val Acc: 0.9633\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 31/31 [00:06<00:00,  5.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [12/15], Loss: 1.0299, Train Acc: 0.9908, Val Loss: 0.0978, Val Acc: 0.9633\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 31/31 [00:05<00:00,  5.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [13/15], Loss: 1.3155, Train Acc: 0.9846, Val Loss: 0.0954, Val Acc: 0.9633\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 31/31 [00:05<00:00,  5.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [14/15], Loss: 1.1616, Train Acc: 0.9857, Val Loss: 0.0939, Val Acc: 0.9633\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 31/31 [00:05<00:00,  5.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [15/15], Loss: 1.3191, Train Acc: 0.9867, Val Loss: 0.1046, Val Acc: 0.9633\nBest model saved with validation accuracy: 0.9673469387755103\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:03<00:00,  3.27it/s]","output_type":"stream"},{"name":"stdout","text":"              image_id      soil_type\n0    img_cdf80d6f.jpeg  Alluvial soil\n1     img_c0142a80.jpg  Alluvial soil\n2     img_91168fb0.jpg  Alluvial soil\n3     img_9822190f.jpg  Alluvial soil\n4    img_e5fc436c.jpeg  Alluvial soil\n..                 ...            ...\n336   img_bc768d49.jpg     Black Soil\n337   img_ddef2a37.jpg     Black Soil\n338   img_be2e7e88.jpg     Black Soil\n339   img_04f21bb9.jpg     Black Soil\n340   img_02c09374.jpg     Black Soil\n\n[341 rows x 2 columns]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1}]}