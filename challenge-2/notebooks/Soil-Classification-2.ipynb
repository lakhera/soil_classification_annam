{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575bac5071ccb1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Soil Image Classification Challenge - Part 2\n",
    "# The Soil Image Classification Challenge is a machine learning competition organised by Annam.ai at IIT Ropar, serving as an initial task for shortlisted hackathon participants.\n",
    "# Task: Classify provided image as Soil or NotSoil image , given Single label Binary Image classification.\n",
    "# Deadline: May 25, 2025, 11:59 PM IST\n",
    "# Team Name: RootCoders\n",
    "# Team Members : Amit Lakhera, Vikramjeet, Pradipta Das, Jyoti Ghungru, Sukanya Saha\n",
    "# -------------------------\n",
    "\n",
    "# Import Required Libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skimage.feature import local_binary_pattern, hog\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, f1_score, accuracy_score, precision_score, recall_score, roc_curve, auc, precision_recall_curve)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define the FeatureExtractor class for extracting features from images\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    A class for extracting features from images using Histogram, Local Binary Pattern (LBP),\n",
    "    and Histogram of Oriented Gradients (HOG).\n",
    "\n",
    "    Attributes:\n",
    "        lbp_radius (int): The radius for the Local Binary Pattern (LBP) feature extraction.\n",
    "        lbp_n_points (int): The number of points for the LBP, calculated as 8 * lbp_radius.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the FeatureExtractor with default parameters for LBP.\n",
    "        \"\"\"\n",
    "        self.lbp_radius = 3\n",
    "        self.lbp_n_points = 8 * self.lbp_radius\n",
    "\n",
    "    def extract_features(self, image_path):\n",
    "        \"\"\"\n",
    "        Extracts a combination of histogram, LBP, and HOG features from a grayscale image for Feature Extraction.\n",
    "\n",
    "        Args:\n",
    "            image_path (str): The file path to the image.\n",
    "        Returns:\n",
    "            np.ndarray: A 1D array containing the concatenated features (histogram, LBP, and HOG).\n",
    "        Raises:\n",
    "            ValueError: If the image cannot be read from the provided path.\n",
    "        \"\"\"\n",
    "        # Read the image in grayscale mode\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Unable to read image: {image_path}\")\n",
    "\n",
    "        # Resize the image to a fixed size of 128x128 pixels\n",
    "        image = cv2.resize(image, (128, 128))\n",
    "\n",
    "        # Extract histogram features\n",
    "        hist = cv2.calcHist([image], [0], None, [256], [0, 256])  # Compute grayscale histogram\n",
    "        hist = cv2.normalize(hist, hist).flatten()  # Normalize and flatten the histogram\n",
    "\n",
    "        # Extract Local Binary Pattern (LBP) features\n",
    "        lbp = local_binary_pattern(image, self.lbp_n_points, self.lbp_radius, method=\"uniform\")\n",
    "        (lbp_hist, _) = np.histogram(\n",
    "            lbp.ravel(),\n",
    "            bins=np.arange(0, self.lbp_n_points + 3),\n",
    "            range=(0, self.lbp_n_points + 2)\n",
    "        )\n",
    "        lbp_hist = lbp_hist.astype(\"float\")\n",
    "        lbp_hist /= (lbp_hist.sum() + 1e-6)  # Normalize the LBP histogram\n",
    "\n",
    "        # Extract Histogram of Oriented Gradients (HOG) features\n",
    "        hog_features = hog(\n",
    "            image,\n",
    "            orientations=9,\n",
    "            pixels_per_cell=(8, 8),\n",
    "            cells_per_block=(2, 2),\n",
    "            block_norm='L2-Hys',\n",
    "            feature_vector=True\n",
    "        )\n",
    "\n",
    "        # Combine all features into a single feature vector\n",
    "        return np.hstack([hist, lbp_hist, hog_features])\n",
    "\n",
    "\n",
    "# SoilClassificationModel class for training and evaluating the One-Class SVM model\n",
    "class SoilClassificationModel:\n",
    "    \"\"\"\n",
    "    A wrapper class for the One-Class SVM model, designed for anomaly detection in soil image datasets.\n",
    "    This class includes methods for loading images, extracting features, training the model,\n",
    "    making predictions, and visualizing results.\n",
    "\n",
    "    Attributes:\n",
    "        model (OneClassSVM): The One-Class SVM model instance.\n",
    "        scaler (StandardScaler): A scaler for normalizing feature data.\n",
    "        feature_extractor (FeatureExtractor): An instance of the FeatureExtractor class\n",
    "            for extracting features from images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel='rbf', gamma='auto', nu=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the SoilClassificationModel with a One-Class SVM model,\n",
    "        a standard scaler for feature normalization, and a feature extractor for image processing.\n",
    "\n",
    "        Args:\n",
    "            kernel (str): Kernel type to be used in the SVM. Defaults to 'rbf'.\n",
    "            gamma (str or float): Kernel coefficient. Defaults to 'auto'.\n",
    "            nu (float): An upper bound on the fraction of training errors and a lower bound of the\n",
    "                        fraction of support vectors. Should be in the interval (0, 1]. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        self.model = OneClassSVM(kernel=kernel, gamma=gamma, nu=nu)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_extractor = FeatureExtractor()\n",
    "\n",
    "    # Load images from a folder and extract features\n",
    "    def load_images_from_folder(self, folder, labels_csv=None):\n",
    "        \"\"\"\n",
    "        Loads images from a specified folder and optionally filters them based on a CSV file.\n",
    "\n",
    "        Args:\n",
    "            folder (str): The path to the folder containing the images.\n",
    "            labels_csv (str, optional): Path to a CSV file containing image_id column.\n",
    "                For training: must contain both image_id and label columns.\n",
    "                For testing: may contain only image_id column.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - np.ndarray: An array of extracted features for each image.\n",
    "                - list: A list of image file names corresponding to the extracted features.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the CSV contains multiple unique labels in training mode.\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        image_files = []\n",
    "\n",
    "        if labels_csv is not None:\n",
    "            # Load the CSV file\n",
    "            try:\n",
    "                df = pd.read_csv(labels_csv)\n",
    "\n",
    "                # Check if this is a test CSV (with only image_id) or training CSV (with image_id and label)\n",
    "                if 'image_id' not in df.columns:\n",
    "                    raise ValueError(\"CSV must contain 'image_id' column\")\n",
    "\n",
    "                is_test_csv = 'label' not in df.columns\n",
    "\n",
    "                if not is_test_csv:\n",
    "                    # Training mode - verify single class for training data\n",
    "                    unique_labels = df['label'].unique()\n",
    "                    if len(unique_labels) > 1:\n",
    "                        raise ValueError(\"Code is trained for Single Binary Classification for Unbalanced data\")\n",
    "                    print(f\"Found {len(df)} image entries in CSV with label: {unique_labels[0]}\")\n",
    "                else:\n",
    "                    # Testing mode\n",
    "                    print(f\"Found {len(df)} image entries in test CSV\")\n",
    "\n",
    "                # Process only the images listed in the CSV\n",
    "                for _, row in df.iterrows():\n",
    "                    filename = row['image_id']\n",
    "                    path = os.path.join(folder, filename)\n",
    "\n",
    "                    if os.path.exists(path):\n",
    "                        try:\n",
    "                            # Extract features from the image\n",
    "                            feat = self.feature_extractor.extract_features(path)\n",
    "                            features.append(feat)\n",
    "                            image_files.append(filename)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Skipping {filename}: {e}\")\n",
    "                    else:\n",
    "                        print(f\"Warning: Image file not found: {path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading CSV file: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            # Original behavior - load all images from folder\n",
    "            for filename in sorted(os.listdir(folder)):\n",
    "                # Check if the file has a valid image extension\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "                    path = os.path.join(folder, filename)\n",
    "                    try:\n",
    "                        # Extract features from the image\n",
    "                        feat = self.feature_extractor.extract_features(path)\n",
    "                        features.append(feat)\n",
    "                        image_files.append(filename)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Skipping {filename}: {e}\")\n",
    "\n",
    "        print(f\"Loaded {len(features)} valid images from {folder}\")\n",
    "        return np.array(features), image_files\n",
    "\n",
    "    # Predict class labels for images in the test folder\n",
    "    def predict(self, test_folder, test_csv=None):\n",
    "        \"\"\"\n",
    "        Predicts the class labels for images in the test folder.\n",
    "\n",
    "        Args:\n",
    "            test_folder (str): Path to the folder containing test images.\n",
    "            test_csv (str, optional): Path to CSV file with image_id column for test images.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - list: Image filenames in the test folder.\n",
    "                - list: Predicted labels (1=soil, 0=not soil) for each image.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If no features can be extracted from the test folder.\n",
    "        \"\"\"\n",
    "        features, image_names = self.load_images_from_folder(test_folder, test_csv)\n",
    "        if features.size == 0:\n",
    "            raise ValueError(\" No features found in test folder.\")\n",
    "        scaled = self.scaler.transform(features)\n",
    "        preds = self.model.predict(scaled)\n",
    "        preds = [1 if p == 1 else 0 for p in preds]  # 1=normal (soil), -1=outlier (not soil) -> convert to 0\n",
    "        return image_names, preds\n",
    "\n",
    "    # Evaluate model predictions with comprehensive metrics and visualizations\n",
    "    def evaluate_predictions(self, true_labels, predicted_labels, plot_results=True):\n",
    "        \"\"\"\n",
    "        Comprehensive evaluation of model predictions with metrics and optional visualizations.\n",
    "\n",
    "        Args:\n",
    "            true_labels (list or np.ndarray): The ground truth labels for the dataset.\n",
    "            predicted_labels (list or np.ndarray): The predicted labels from the model.\n",
    "            plot_results (bool, optional): Whether to generate and display visualizations. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the calculated metrics:\n",
    "                - 'Accuracy': The accuracy of the predictions.\n",
    "                - 'Precision': The precision of the predictions.\n",
    "                - 'Recall': The recall of the predictions.\n",
    "                - 'F1 Score': The F1 score of the predictions.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\" COMPREHENSIVE MODEL EVALUATION\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "        precision = precision_score(true_labels, predicted_labels)\n",
    "        recall = recall_score(true_labels, predicted_labels)\n",
    "        f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "        # Print detailed metrics\n",
    "        print(f\"\\n PERFORMANCE METRICS:\")\n",
    "        print(f\"   Accuracy:  {accuracy:.4f}\")\n",
    "        print(f\"   Precision: {precision:.4f}\")\n",
    "        print(f\"   Recall:    {recall:.4f}\")\n",
    "        print(f\"   F1 Score:  {f1:.4f}\")\n",
    "\n",
    "        print(f\"\\n CLASSIFICATION REPORT:\")\n",
    "        print(classification_report(true_labels, predicted_labels, digits=4))\n",
    "\n",
    "        print(f\"\\n CONFUSION MATRIX:\")\n",
    "        cm = confusion_matrix(true_labels, predicted_labels)\n",
    "        print(cm)\n",
    "\n",
    "        # Create metrics dictionary for plotting\n",
    "        metrics_dict = {\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1\n",
    "        }\n",
    "\n",
    "        if plot_results:\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"\\n GENERATING VISUALIZATIONS...\")\n",
    "            print(\"=\" * 60)\n",
    "            self.plot_confusion_matrix(true_labels, predicted_labels)\n",
    "            self.plot_metrics_comparison(metrics_dict)\n",
    "            self.plot_prediction_distribution(predicted_labels)\n",
    "\n",
    "        return metrics_dict\n",
    "\n",
    "    # Fit the model on training data\n",
    "    def fit(self, target_folder, labels_csv=None):\n",
    "        \"\"\"\n",
    "        Trains the model on images from the specified folder.\n",
    "\n",
    "        Args:\n",
    "            target_folder (str): Path to the folder containing training images.\n",
    "            labels_csv (str, optional): Path to CSV file with image_id and label columns.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If no features can be extracted from the training folder.\n",
    "        \"\"\"\n",
    "        features, _ = self.load_images_from_folder(target_folder, labels_csv)\n",
    "        if features.size == 0:\n",
    "            raise ValueError(\" No features found in training folder.\")\n",
    "        scaled = self.scaler.fit_transform(features)\n",
    "        self.model.fit(scaled)\n",
    "        print(\" Model trained successfully.\")\n",
    "\n",
    "    def train_and_evaluate_model(self, train_folder, test_folder, train_csv=None, test_csv=None, validation_split=0.2, random_state=42):\n",
    "        \"\"\"\n",
    "        Trains the model using the provided training data, evaluates it on a validation set (if specified),\n",
    "        and tests it on the test data. Generates evaluation metrics, visualizations, and saves predictions.\n",
    "\n",
    "        Args:\n",
    "            train_folder (str): Path to the folder containing training images.\n",
    "            test_folder (str): Path to the folder containing test images.\n",
    "            train_csv (str, optional): Path to CSV file with training image_id and label columns.\n",
    "            test_csv (str, optional): Path to CSV file with test image_id column.\n",
    "            validation_split (float, optional): Proportion of training data to use for validation. Defaults to 0.2.\n",
    "            random_state (int, optional): Random seed for reproducibility of the train-validation split. Defaults to 42.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - SoilClassificationModel: The trained classifier instance.\n",
    "                - dict or None: Validation metrics if validation_split > 0, otherwise None.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If no training data is found in the specified folder.\n",
    "        \"\"\"\n",
    "        print(\" STARTING SOIL CLASSIFICATION TRAINING AND EVALUATION\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Load training data\n",
    "        print(f\"\\n Loading training data from: {train_folder}\")\n",
    "        if train_csv:\n",
    "            print(f\" Using labels from: {train_csv}\")\n",
    "\n",
    "        train_features, train_files = self.load_images_from_folder(train_folder, train_csv)\n",
    "\n",
    "        if len(train_features) == 0:\n",
    "            raise ValueError(\" No training data found!\")\n",
    "\n",
    "        # Split training data for validation\n",
    "        val_metrics = None\n",
    "        if validation_split > 0:\n",
    "            print(f\"\\n Splitting training data (validation split: {validation_split})\")\n",
    "            train_feat, val_feat, train_names, val_names = train_test_split(\n",
    "                train_features, train_files, test_size=validation_split,\n",
    "                random_state=random_state\n",
    "            )\n",
    "\n",
    "            # Train on training subset\n",
    "            print(f\" Training samples: {len(train_feat)}\")\n",
    "            print(f\" Validation samples: {len(val_feat)}\")\n",
    "\n",
    "            # Fit scaler and model on training data\n",
    "            scaled_train = self.scaler.fit_transform(train_feat)\n",
    "            self.model.fit(scaled_train)\n",
    "            print(\" Model trained on training subset.\")\n",
    "\n",
    "            # Validate on validation set\n",
    "            scaled_val = self.scaler.transform(val_feat)\n",
    "            val_preds = self.model.predict(scaled_val)\n",
    "            val_preds = [1 if p == 1 else 0 for p in val_preds]\n",
    "\n",
    "            # Create true labels for validation (all should be 1 since they're from training folder)\n",
    "            val_true_labels = [1] * len(val_preds)\n",
    "\n",
    "            print(f\"\\n VALIDATION RESULTS:\")\n",
    "            val_metrics = self.evaluate_predictions(val_true_labels, val_preds, plot_results=True)\n",
    "        else:\n",
    "            # Train on all data\n",
    "            scaled_train = self.scaler.fit_transform(train_features)\n",
    "            self.model.fit(scaled_train)\n",
    "            print(\" Model trained on all training data.\")\n",
    "\n",
    "        # Test on test folder\n",
    "        print(f\"\\n Testing on: {test_folder}\")\n",
    "        if test_csv:\n",
    "            print(f\" Using test IDs from: {test_csv}\")\n",
    "        image_ids, test_predictions = self.predict(test_folder, test_csv)\n",
    "\n",
    "        # Save submission\n",
    "        df = pd.DataFrame({\"image_id\": image_ids, \"label\": test_predictions})\n",
    "        df.to_csv(\"submission.csv\", index=False)\n",
    "        print(\" Saved submission.csv\")\n",
    "\n",
    "        # Show test predictions distribution\n",
    "        self.plot_prediction_distribution(test_predictions, \"test_predictions_distribution.png\")\n",
    "\n",
    "        # Show sample images with predictions\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(f\" Generating sample image predictions...\")\n",
    "        print(\"=\" * 50)\n",
    "        self.plot_sample_images_with_predictions(test_folder, image_ids, test_predictions,\n",
    "                                                 num_samples=20, save_path=\"sample_test_predictions.png\")\n",
    "        return self, val_metrics\n",
    "\n",
    "    # Plot confusion matrix heatmap\n",
    "    \n",
    "    def plot_confusion_matrix(self, true_labels, predicted_labels, save_path=\"confusion_matrix.png\"):\n",
    "        \"\"\"\n",
    "        Plot and save confusion matrix heatmap.\n",
    "\n",
    "        Args:\n",
    "            true_labels (list or np.ndarray): The ground truth labels for the dataset.\n",
    "            predicted_labels (list or np.ndarray): The predicted labels from the model.\n",
    "            save_path (str, optional): Path where the plot image will be saved. Defaults to \"confusion_matrix.png\".\n",
    "        \"\"\"\n",
    "        cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=['NotSoil', 'Soil'],\n",
    "                    yticklabels=['NotSoil', 'Soil'])\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\" Confusion matrix saved to {save_path}\")\n",
    "\n",
    "    # Plot comparison of different metrics\n",
    "\n",
    "    def plot_metrics_comparison(self, metrics_dict, save_path=\"metrics_comparison.png\"):\n",
    "        \"\"\"\n",
    "        Plot comparison of different metrics.\n",
    "\n",
    "        Args:\n",
    "            metrics_dict (dict): Dictionary containing metric names as keys and their values.\n",
    "            save_path (str, optional): Path where the plot image will be saved. Defaults to \"metrics_comparison.png\".\n",
    "        \"\"\"\n",
    "        metrics = list(metrics_dict.keys())\n",
    "        values = list(metrics_dict.values())\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(metrics, values, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])\n",
    "        plt.title('Model Performance Metrics')\n",
    "        plt.ylabel('Score')\n",
    "        plt.ylim(0, 1)\n",
    "\n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, values):\n",
    "            plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,\n",
    "                     f'{value:.3f}', ha='center', va='bottom')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\" Metrics comparison saved to {save_path}\")\n",
    "\n",
    "    # Plot distribution of predictions\n",
    "\n",
    "    def plot_prediction_distribution(self, predicted_labels, save_path=\"prediction_distribution.png\"):\n",
    "        \"\"\"\n",
    "        Plot distribution of predictions.\n",
    "\n",
    "        Args:\n",
    "            predicted_labels (list or np.ndarray): The predicted labels from the model.\n",
    "            save_path (str, optional): Path where the plot image will be saved. Defaults to \"prediction_distribution.png\".\n",
    "        \"\"\"\n",
    "        unique, counts = np.unique(predicted_labels, return_counts=True)\n",
    "\n",
    "        # Create a dictionary mapping values to their counts\n",
    "        counts_dict = dict(zip(unique, counts))\n",
    "\n",
    "        # Get counts for each category (0 and 1), default to 0 if not present\n",
    "        not_soil_count = counts_dict.get(0, 0)\n",
    "        soil_count = counts_dict.get(1, 0)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        bars = plt.bar(['NotSoil', 'Soil'], [not_soil_count, soil_count], color=['lightcoral', 'lightgreen'])\n",
    "        plt.title('Distribution of Predictions')\n",
    "        plt.ylabel('Count')\n",
    "\n",
    "        # Add value labels on bars\n",
    "        for bar, count in zip(bars, [not_soil_count, soil_count]):\n",
    "            plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5,\n",
    "                     str(count), ha='center', va='bottom')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\" Prediction distribution saved to {save_path}\")\n",
    "\n",
    "    \n",
    "    # Plot sample images with their predictions\n",
    "\n",
    "    def plot_sample_images_with_predictions(self, folder_path, image_names, predictions, num_samples=8, save_path=\"sample_predictions.png\"):\n",
    "        \"\"\"\n",
    "        Plot sample images with their predictions.\n",
    "\n",
    "        Args:\n",
    "            folder_path (str): Path to the folder containing images.\n",
    "            image_names (list): List of image file names.\n",
    "            predictions (list): List of predictions corresponding to the images.\n",
    "            num_samples (int, optional): Number of sample images to plot. Defaults to 20.\n",
    "            save_path (str, optional): Path where the plot image will be saved. Defaults to \"sample_predictions.png\".\n",
    "        \"\"\"\n",
    "        if len(image_names) < num_samples:\n",
    "            num_samples = len(image_names)\n",
    "\n",
    "        # Select random samples\n",
    "        indices = np.random.choice(len(image_names), num_samples, replace=False)\n",
    "\n",
    "        # Calculate grid dimensions - make it more flexible\n",
    "        grid_size = int(np.ceil(np.sqrt(num_samples)))\n",
    "        fig, axes = plt.subplots(grid_size, grid_size, figsize=(14, 14))  # Reduced figure size\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for i, idx in enumerate(indices):\n",
    "            if i >= len(axes):  # Safety check\n",
    "                break\n",
    "\n",
    "            img_path = os.path.join(folder_path, image_names[idx])\n",
    "            try:\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is not None:\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    axes[i].imshow(img)\n",
    "\n",
    "                    # Set title with prediction - shorter titles\n",
    "                    pred_label = \"Soil\" if predictions[idx] == 1 else \"NotSoil\"\n",
    "                    color = \"green\" if predictions[idx] == 1 else \"red\"\n",
    "                    axes[i].set_title(f\"{image_names[idx][:10]}...\\n{pred_label}\", color=color, fontsize=8)  # Smaller font size\n",
    "                    axes[i].axis('off')\n",
    "                else:\n",
    "                    axes[i].text(0.5, 0.5, 'Image\\nNot Found', ha='center', va='center')\n",
    "                    axes[i].set_title(f\"{image_names[idx][:10]}...\")\n",
    "                    axes[i].axis('off')\n",
    "            except Exception as e:\n",
    "                axes[i].text(0.5, 0.5, f'Error', ha='center', va='center')\n",
    "                axes[i].set_title(f\"{image_names[idx][:10]}...\")\n",
    "                axes[i].axis('off')\n",
    "\n",
    "        # Turn off any unused subplots\n",
    "        for i in range(len(indices), len(axes)):\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        plt.tight_layout(pad=0.5)  # Reduce padding between subplots\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\" Sample predictions saved to {save_path}\")\n",
    "\n",
    "    # Save the trained model to a file using pickle\n",
    "\n",
    "    def save_model(self, filepath=\"soil_classification_model.pkl\"):\n",
    "        \"\"\"\n",
    "        Save the trained model to a file using pickle.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path where the model will be saved.\n",
    "        \"\"\"\n",
    "        import pickle\n",
    "        model_data = {\n",
    "            'model': self.model,\n",
    "            'scaler': self.scaler,\n",
    "            'feature_extractor': self.feature_extractor\n",
    "        }\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        print(f\" Model saved to {filepath}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, filepath=\"soil_classification_model.pkl\"):\n",
    "        \"\"\"\n",
    "        Load a trained model from a file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to the saved model file.\n",
    "        Returns:\n",
    "            SoilClassificationModel: A loaded model instance.\n",
    "        \"\"\"\n",
    "        import pickle\n",
    "        with open(filepath, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "\n",
    "        instance = cls()\n",
    "        instance.model = model_data['model']\n",
    "        instance.scaler = model_data['scaler']\n",
    "        instance.feature_extractor = model_data['feature_extractor']\n",
    "        print(f\" Model loaded from {filepath}\")\n",
    "        return instance\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage paths - update these to your actual paths\n",
    "    train_folder = \"/kaggle/input/soil-classification-part-2/soil_competition-2025/train\"\n",
    "    test_folder = \"/kaggle/input/soil-classification-part-2/soil_competition-2025/test\"\n",
    "    train_csv = \"/kaggle/input/soil-classification-part-2/soil_competition-2025/train_labels.csv\"\n",
    "    test_csv = \"/kaggle/input/soil-classification-part-2/soil_competition-2025/test_ids.csv\"\n",
    "\n",
    "    try:\n",
    "        # Initialize and train the model\n",
    "        model = SoilClassificationModel(nu=0.1)  # Adjust parameters as needed\n",
    "\n",
    "        # Train and evaluate\n",
    "        trained_model, validation_metrics = model.train_and_evaluate_model(\n",
    "            train_folder=train_folder,\n",
    "            test_folder=test_folder,\n",
    "            train_csv=train_csv,\n",
    "            test_csv=test_csv,\n",
    "            validation_split=0.2\n",
    "        )\n",
    "\n",
    "        # Save the trained model\n",
    "        model.save_model(\"soil_classification_model.pkl\")\n",
    "\n",
    "        print(\"\\n TRAINING AND EVALUATION COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"Check the generated plots and submission.csv file for results.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error: {e}\")\n",
    "        print(\"Please check that the folder paths exist and contain valid images.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
